{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEK2QYPrX78R"
      },
      "source": [
        "# Assignment 01: Supervised learning, Linear models, and Loss functions\n",
        "\n",
        "In this assignment, you're going to write your own methods to fit a linear model using either an OLS or Huber loss function.  \n",
        "\n",
        "## Data set\n",
        "\n",
        "We will examine some data representing the miles-per-gallon of 398 cars given other variables describing them:\n",
        "\n",
        "1. mpg: continuous. The miles-per-gallon of the car.\n",
        "2. cylinders: multi-valued discrete. Number of cylinders.\n",
        "3. displacement: continuous. Engine displacement of the car.\n",
        "4. horsepower: continuous. Total horsepower of the car.\n",
        "5. weight: continuous. Weight in lbs.\n",
        "6. acceleration: continuous. Acceleration 0-60mph in seconds.\n",
        "9. car name: string (unique for each instance)\n",
        "\n",
        "## Follow These Steps Before Submitting\n",
        "Once you are finished, ensure to complete the following steps.\n",
        "\n",
        "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
        "\n",
        "2.  Fix any errors which result from this.\n",
        "\n",
        "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
        "\n",
        "4.  Submit your completed notebook to OWL by the deadline.\n",
        "\n",
        "\n",
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBu-equFX78Y"
      },
      "outputs": [],
      "source": [
        "# Import all the necessary packages:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as ss\n",
        "import scipy.optimize as so\n",
        "from sklearn import linear_model\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBv9WDghTVHx"
      },
      "outputs": [],
      "source": [
        "# Uncomment if using Google Colab or Kaggle Kernels.\n",
        "# Imports the data using gdown.\n",
        "!gdown https://drive.google.com/uc?id=1PtY3ne37XA8Jk_cAf0Cd7JSRvEU8KDbp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8CtqKgpX78f"
      },
      "source": [
        "\n",
        "## Part 1\n",
        "### Question 1.1:\n",
        "\n",
        "\n",
        "Read the `car_data.csv` file as a `pandas.DataFrame` and show its descriptive statistics.  Investigate the relationship between the cars' weight and their mpg by plotting a scatter plot of the `weight` (x axis) and `mpg` columns (y axis). Add an `alpha`(transparency of the plotted dots) in case some data are overlapping. Remember to label your axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdFNvFsRUdx9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('car_data.csv')\n",
        "\n",
        "print(df.describe())\n",
        "print(\"\\ndata shape:\", df.shape)\n",
        "print(\"\\nfirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['weight'], df['MPG'], alpha=0.5)\n",
        "plt.xlabel('weight (lbs)')\n",
        "plt.ylabel('mpg (miles per gallon)')\n",
        "plt.title('relationship between car weight and mpg')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUVK0ZGmTAcE"
      },
      "source": [
        "**Written answer: What do you see here? Discuss your findings**\n",
        "I can observe a strong negative relationship between car weight and mpg. As the weight of the car increases, the mpg decreases. This makes sense because heavy cars need more energy to move, making fuel efficiency lower. The relationship is linear and lacks clear outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLTkjzp2X78w"
      },
      "source": [
        "### Question 1.2:\n",
        "\n",
        "Recall that the linear model, we obtain predictions by computing\n",
        "\n",
        "$$ \\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\beta} $$\n",
        "\n",
        "Here, $\\mathbf{X}$ is a design matrix which includes a column of ones, $\\hat{\\beta}$ are coefficients, and $\\hat{\\mathbf{y}}$ are outcomes.  Write a function `linearModelPredict` to compute linear model predictions given data and a coefficient vector.  The function should take as it's arguments a 1d-array of coefficients `b` and the design matrix `X` as a 2d-array and return linear model predictions `yp`.\n",
        "\n",
        "Test the function by setting\n",
        "\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "```\n",
        "\n",
        "Call your function using these values.\n",
        "\n",
        "Report $\\hat{\\mathbf{y}}$.\n",
        "\n",
        "What is the dimensionality of the numpy-array that you get back?\n",
        "\n",
        "Hint:  Read the documentation for `np.dot` or the `@` operator in `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So7kD_OPVQNI"
      },
      "outputs": [],
      "source": [
        "def linearModelPredict(b, X):\n",
        "    yp = X @ b\n",
        "    return yp\n",
        "\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "yp = linearModelPredict(b, X)\n",
        "\n",
        "print(\"predictions (y_hat):\", yp)\n",
        "print(\"dimensionality:\", yp.shape)\n",
        "print(\"type:\", type(yp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrZokVidX787"
      },
      "source": [
        "### Question 1.3:\n",
        "\n",
        "Write a function `linearModelMSE` which computes and returns the mean squared error parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes. Recall that:\n",
        "\n",
        "$$\n",
        "MSE(y_i, \\hat{y_i}) = \\frac{1}{|I|} \\sum_i (y_i - \\hat{y_i})^2\n",
        "$$\n",
        "$$\n",
        "\\Delta MSE(y, \\hat{y}) = -\\frac{2}{|I|} \\left[(y-\\hat{y}) \\cdot X\\right]\n",
        "$$\n",
        "\n",
        "Test the function with the values\n",
        "\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2])\n",
        "```\n",
        "\n",
        "Report the loss and the gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHuS9PJJVZiW"
      },
      "outputs": [],
      "source": [
        "def linearModelMSE(beta, X, y):\n",
        "    n = len(y)\n",
        "    y_pred = X @ beta\n",
        "    residuals = y - y_pred\n",
        "    loss = np.mean(residuals**2)\n",
        "    gradient = -2 / n * (residuals @ X)\n",
        "    return loss, gradient\n",
        "\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2])\n",
        "\n",
        "loss, gradient = linearModelMSE(b, X, y)\n",
        "print(\"loss (mse):\", loss)\n",
        "print(\"gradient:\", gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7tEGnLsVb4L"
      },
      "source": [
        "**Written answer**: To minimize the loss, do you need increase or decrease the value of the parameters?\n",
        "\n",
        "To minimize the loss, we need to move in the direction opposite to the gradient. Since the gradient is negative, we should increase the parameters. More generally, we update parameters as: β_new = β_old - learning_rate * gradient. Since the gradient is negative, subtracting it (which means adding a positive value) will increase the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpfl2A86X79D"
      },
      "source": [
        "### Question 1.4:\n",
        "\n",
        "Now that you've implemented a loss function in question 1.3, it is now time to minimize it!\n",
        "\n",
        "Write a function `linearModelFit` to fit a linear model.  The function should take as its first argument the design matrix `X` as a 2d-array, as its second argument a 1d-array `y` of outcomes, and as its third argument a function  `lossfcn` which returns as a tuple the value of the loss, as well as the gradient of the loss. As a result, it should return the estimated betas and the R2.\n",
        "\n",
        "Test the function with the values:\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "y = np.array([0,0.4,2])\n",
        "```\n",
        "\n",
        "Report the best parameters and the fitted $R^2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS9RD3VIsYRy"
      },
      "outputs": [],
      "source": [
        "def linearModelFit(X, y, lossfcn):\n",
        "    \n",
        "    n_features = X.shape[1]\n",
        "    beta_init = np.zeros(n_features)\n",
        "    \n",
        "    def objective(beta):\n",
        "        loss, _ = lossfcn(beta, X, y)\n",
        "        return loss\n",
        "    \n",
        "    def gradient(beta):\n",
        "        _, grad = lossfcn(beta, X, y)\n",
        "        return grad\n",
        "    \n",
        "    result = so.minimize(objective, beta_init, method='L-BFGS-B', jac=gradient)\n",
        "    beta = result.x\n",
        "    \n",
        "    y_pred = X @ beta\n",
        "    ss_res = np.sum((y - y_pred)**2)\n",
        "    ss_tot = np.sum((y - np.mean(y))**2)\n",
        "    R2 = 1 - (ss_res / ss_tot)\n",
        "    \n",
        "    return beta, R2\n",
        "\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "y = np.array([0,0.4,2])\n",
        "\n",
        "beta, R2 = linearModelFit(X, y, linearModelMSE)\n",
        "print(\"best parameters (beta):\", beta)\n",
        "print(\"r-squared:\", R2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ2ZergqX79L"
      },
      "source": [
        "### Question 1.5:\n",
        "\n",
        "Use the above functions to fit your model to the car data. Use the MPG as the target (y) variable and the weight as the independent (x). Then use your model and the fitted parameters to make predictions along a grid of equally spaced weights within the original range of the weight variable.  \n",
        "\n",
        "Plot the data and add a line for the predicted values. You can get these by generating a new X-matrix with 100 equally space weights (using for example [```np.linspace```](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)). Also report the $R^2$ value for the fit. You can do this by either printing out the $R^2$ of the fit or putting it on your plot via the `annotate` function in matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgN67l1csYRz"
      },
      "outputs": [],
      "source": [
        "X_data = df[['weight']].values\n",
        "y_data = df['MPG'].values\n",
        "\n",
        "X_design = np.column_stack([np.ones(len(X_data)), X_data])\n",
        "\n",
        "beta_fit, R2_fit = linearModelFit(X_design, y_data, linearModelMSE)\n",
        "print(f\"fitted coefficients: intercept = {beta_fit[0]:.4f}, weight = {beta_fit[1]:.4f}\")\n",
        "print(f\"r-squared: {R2_fit:.4f}\")\n",
        "\n",
        "weight_min = df['weight'].min()\n",
        "weight_max = df['weight'].max()\n",
        "weight_grid = np.linspace(weight_min, weight_max, 100)\n",
        "\n",
        "X_grid = np.column_stack([np.ones(len(weight_grid)), weight_grid])\n",
        "\n",
        "y_pred = linearModelPredict(beta_fit, X_grid)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['weight'], df['MPG'], alpha=0.5, label='data')\n",
        "plt.plot(weight_grid, y_pred, 'r-', linewidth=2, label=f'fitted line (r² = {R2_fit:.4f})')\n",
        "plt.xlabel('weight (lbs)')\n",
        "plt.ylabel('mpg (miles per gallon)')\n",
        "plt.title('linear model fit: weight vs mpg')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoEdIRYA7h3l"
      },
      "source": [
        "### Question 1.6:\n",
        "\n",
        "Now use sklearn's `linear_model` to fit the model with all the available data. Plot the data and add a line for the predicted values as you did in the previous question. Also report the $R^2$ value for the fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-R5zFcS7h3l"
      },
      "outputs": [],
      "source": [
        "feature_cols = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']\n",
        "X_all = df[feature_cols].values\n",
        "y_all = df['MPG'].values\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X_all, y_all)\n",
        "\n",
        "R2_sklearn = model.score(X_all, y_all)\n",
        "print(f\"sklearn r-squared (all features): {R2_sklearn:.4f}\")\n",
        "print(f\"intercept: {model.intercept_:.4f}\")\n",
        "print(\"coefficients:\")\n",
        "for col, coef in zip(feature_cols, model.coef_):\n",
        "    print(f\"  {col}: {coef:.4f}\")\n",
        "\n",
        "weight_grid = np.linspace(df['weight'].min(), df['weight'].max(), 100)\n",
        "X_grid_all = np.zeros((len(weight_grid), len(feature_cols)))\n",
        "for i, col in enumerate(feature_cols):\n",
        "    if col == 'weight':\n",
        "        X_grid_all[:, i] = weight_grid\n",
        "    else:\n",
        "        X_grid_all[:, i] = df[col].mean()\n",
        "\n",
        "y_pred_all = model.predict(X_grid_all)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['weight'], df['MPG'], alpha=0.5, label='data')\n",
        "plt.plot(weight_grid, y_pred_all, 'g-', linewidth=2, label=f'sklearn fit (r² = {R2_sklearn:.4f})')\n",
        "plt.xlabel('weight (lbs)')\n",
        "plt.ylabel('mpg (miles per gallon)')\n",
        "plt.title('linear model fit with all features (weight vs mpg projection)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1tujUgc7h3m"
      },
      "source": [
        "**Written answer: How much do you gain by having more variables?**\n",
        "\n",
        "By including all available variables instead of just weight, we get better at predicting. The R² value increases from 0.69 (with just weight) to 0.81 (with all features). This implies that other car characteristics (not just weight) contribute to fuel efficiency. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3emHgAcX79R"
      },
      "source": [
        "\n",
        "### Part 2: Robust Regression with Huber Loss\n",
        "\n",
        "In Part 1, we minimized the Sum of Squared Errors (OLS). While OLS is computationally efficient, it is **sensitive to outliers**. To fix this, we will use **Huber Loss**, which combines the best properties of Squared Error (near the mean) and Absolute Error (for outliers).\n",
        "\n",
        "#### Question 2.1:\n",
        "\n",
        "First, we need to define the Huber loss function mathematically in code and test the results. The function is defined as:\n",
        "\n",
        "$$\n",
        "L_{\\delta}(y, f(x)) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{2}(y - f(x))^2 & \\text{for } |\\frac{y - f(x)}{\\sigma}| \\le \\delta, \\\\\n",
        "\\delta (|y - f(x)| - \\frac{1}{2}\\delta), & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The parameter $\\sigma$ makes sure that if y is scaled up or down by a certain factor, one does not need to rescale $\\delta$ to achieve the same robustness. $\\delta$ is a threshold parameter that determines the point where the loss function transitions from quadratic to linear, or in other words, the point where we start treating errors as outliers. This can be adjusted based on how much robustness you want. $\\delta$ must be greater than 1. Parameters such as $\\delta$ are commonly known as **hyperparameters**,\n",
        " parameters that are not learned from the data but set (by the user) prior to training.\n",
        "\n",
        "**Your Task:**\n",
        "1.  Write a function `huber_loss(y_true, y_pred, delta)` that calculates the loss for a set of predictions.\n",
        "2.  Write an `objective_function(weights)` that calculates the **sum** of the Huber loss over the entire dataset. Use $\\delta = 1.35$.\n",
        "3.  Use `scipy.optimize.minimize` to find the weights (coefficients) that minimize this loss. Use `method = 'L-BFGS-B'` in the minimize function, to ensure convergence. This function is harder to optimize, so the standard methods may not work well. L-BFGS-B is a quasi-Newton method that can handle large-scale problems efficiently and normally converges faster than derivative-free methods. Give a starting value of 1 for all weights including sigma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Rbz9MpsYR0"
      },
      "outputs": [],
      "source": [
        "def huber_loss(y_true, y_pred, delta, sigma):\n",
        "    residuals = (y_true - y_pred) / sigma\n",
        "    abs_residuals = np.abs(residuals)\n",
        "    \n",
        "    quadratic_mask = abs_residuals <= delta\n",
        "    quadratic_loss = 0.5 * (residuals[quadratic_mask])**2\n",
        "    \n",
        "    linear_mask = ~quadratic_mask\n",
        "    linear_loss = delta * (abs_residuals[linear_mask] - 0.5 * delta)\n",
        "    \n",
        "    loss = np.zeros_like(residuals)\n",
        "    loss[quadratic_mask] = quadratic_loss\n",
        "    loss[linear_mask] = linear_loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "X_huber = df[['weight']].values\n",
        "y_huber_target = df['MPG'].values\n",
        "\n",
        "X_huber_design = np.column_stack([np.ones(len(X_huber)), X_huber])\n",
        "\n",
        "delta = 1.35\n",
        "n_samples = len(y_huber_target)\n",
        "\n",
        "def objective_function(weights):\n",
        "    sigma = weights[0]\n",
        "    beta = weights[1:]\n",
        "    \n",
        "    y_pred = X_huber_design @ beta\n",
        "    losses = huber_loss(y_huber_target, y_pred, delta, sigma)\n",
        "    total_loss = np.sum(losses)\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "initial_weights = np.array([1.0, 1.0, 1.0])\n",
        "\n",
        "result = so.minimize(objective_function, initial_weights, method='L-BFGS-B')\n",
        "\n",
        "huber_sigma = result.x[0]\n",
        "huber_intercept = result.x[1]\n",
        "huber_coeffs = result.x[2:]\n",
        "\n",
        "print(\"optimization result:\")\n",
        "print(f\"sigma: {huber_sigma:.4f}\")\n",
        "print(f\"intercept: {huber_intercept:.4f}\")\n",
        "print(f\"coefficient (weight): {huber_coeffs[0]:.4f}\")\n",
        "print(f\"total loss: {result.fun:.4f}\")\n",
        "print(f\"optimization successful: {result.success}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTx_xIR5X79V"
      },
      "source": [
        "### Question 2.2:\n",
        "\n",
        "Now that we have our optimized weights, let's make predictions and visualize the result.\n",
        "\n",
        "**Your Task:**\n",
        "1. Calculate the predictions `y_huber` using your `huber_intercept` and `huber_coeffs`.\n",
        "2. Plot `y_huber` against the original target variable `y` to check the fit.\n",
        "3. Calculate and report the $R^2$ value for the Huber regression fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzXl2mW0sYR1"
      },
      "outputs": [],
      "source": [
        "y_huber = X_huber_design @ np.concatenate([[huber_intercept], huber_coeffs])\n",
        "\n",
        "ss_res = np.sum((y_huber_target - y_huber)**2)\n",
        "ss_tot = np.sum((y_huber_target - np.mean(y_huber_target))**2)\n",
        "R2_huber = 1 - (ss_res / ss_tot)\n",
        "\n",
        "print(f\"huber regression r-squared: {R2_huber:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_huber_target, y_huber, alpha=0.5)\n",
        "plt.plot([y_huber_target.min(), y_huber_target.max()], \n",
        "         [y_huber_target.min(), y_huber_target.max()], \n",
        "         'r--', linewidth=2, label='perfect prediction')\n",
        "plt.xlabel('actual mpg')\n",
        "plt.ylabel('predicted mpg (huber)')\n",
        "plt.title(f'huber regression: predicted vs actual mpg (r² = {R2_huber:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "weight_grid = np.linspace(df['weight'].min(), df['weight'].max(), 100)\n",
        "X_grid_huber = np.column_stack([np.ones(len(weight_grid)), weight_grid])\n",
        "y_huber_grid = X_grid_huber @ np.concatenate([[huber_intercept], huber_coeffs])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['weight'], df['MPG'], alpha=0.5, label='data')\n",
        "plt.plot(weight_grid, y_huber_grid, 'b-', linewidth=2, label=f'huber fit (r² = {R2_huber:.4f})')\n",
        "plt.xlabel('weight (lbs)')\n",
        "plt.ylabel('mpg (miles per gallon)')\n",
        "plt.title('huber regression: weight vs mpg')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYbaeWI5X79Y"
      },
      "source": [
        "**Written answer: How does the Huber regression fit compare to the OLS fit from Part 1? Discuss your findings.**\n",
        "\n",
        "Huber regression and OLS give similar results when there are no outliers. Huber regression is better at handling outliers because it doesn't let outliers affect the trend line as much. The R² values are usually similar, but Huber can work better when outliers exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tza7fD4X79a"
      },
      "source": [
        "#### Question 2.3:\n",
        "\n",
        "To ensure our manual implementation is correct, we should compare it with the industry-standard implementation from `scikit-learn`.\n",
        "\n",
        "**Your Task:**\n",
        "1. Fit a `HuberRegressor` from `sklearn.linear_model`. Note the $\\delta$ parameter is called `epsilon` in sklearn.\n",
        "2. Compare the $R^2$ scores.\n",
        "3. Plot the predictions from your manual implementation against the sklearn implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hb7fMulsYR1"
      },
      "outputs": [],
      "source": [
        "huber_sklearn = linear_model.HuberRegressor(epsilon=delta, max_iter=200)\n",
        "huber_sklearn.fit(X_huber, y_huber_target)\n",
        "\n",
        "y_huber_sklearn = huber_sklearn.predict(X_huber)\n",
        "\n",
        "R2_sklearn_huber = huber_sklearn.score(X_huber, y_huber_target)\n",
        "\n",
        "print(\"sklearn huberregressor:\")\n",
        "print(f\"intercept: {huber_sklearn.intercept_:.4f}\")\n",
        "print(f\"coefficient (weight): {huber_sklearn.coef_[0]:.4f}\")\n",
        "print(f\"r-squared: {R2_sklearn_huber:.4f}\")\n",
        "\n",
        "print(\"\\nmanual:\")\n",
        "print(f\"intercept: {huber_intercept:.4f}\")\n",
        "print(f\"coefficient (weight): {huber_coeffs[0]:.4f}\")\n",
        "print(f\"r-squared: {R2_huber:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_huber, y_huber_sklearn, alpha=0.5)\n",
        "plt.plot([y_huber.min(), y_huber.max()], \n",
        "         [y_huber.min(), y_huber.max()], \n",
        "         'r--', linewidth=2, label='perfect agreement')\n",
        "plt.xlabel('manual predictions')\n",
        "plt.ylabel('sklearn predictions')\n",
        "plt.title('comparison: manual vs sklearn huber regression')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "correlation = np.corrcoef(y_huber, y_huber_sklearn)[0, 1]\n",
        "print(f\"\\ncorrelation between predictions: {correlation:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQJl6CwAl9hu"
      },
      "source": [
        "**Written answer: Comment on the similarity (or lack thereof) between the two implementations.**\n",
        "\n",
        "The two implementations should give similar results with high correlation between predictions. They might be slightly different because sklearn uses different methods to find the best answer, or because of small rounding errors in calculations. If the results are very similar with similar R² values, it means our manual implementation is working right. If they are very different, there might be a problem with our code or how we set up the loss function."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
