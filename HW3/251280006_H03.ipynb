{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7_BO2jYnC1ge",
      "metadata": {
        "id": "7_BO2jYnC1ge"
      },
      "source": [
        "# Assignment 3: Logistic Regression\n",
        "\n",
        "## Instructions\n",
        "\n",
        "* Complete the assignment as outlined below.\n",
        "* Restart your kernel and rerun your cells before submission.\n",
        "* Submit your completed notebook (.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmkN-Z_BC1gg",
      "metadata": {
        "id": "mmkN-Z_BC1gg"
      },
      "source": [
        "## Dataset Information\n",
        "\n",
        "The dataset includes information on a total of 1000 burn related hospitalizations. The outcome of interest is survival to hospital discharge (`death`).\n",
        "Below are the features:\n",
        "\n",
        "* `death`: Hospital discharge status (1 = Dead, 0 = Alive)\n",
        "* `age`: Age at admission (Years)\n",
        "* `gender`: Gender (1 = Male, 0 = Female)\n",
        "* `tbsa`: Total burn surface area (Minor, Moderate, Severe, Critical)\n",
        "* `race`: Race (1 = White, 0 = Non-White)\n",
        "* `inh_inj`: Burn involved inhalation injury (1 = Yes, 0 = No)\n",
        "* `flame`: Flame involved in burn injury (1 = Yes, 0 = No)\n",
        "\n",
        "Your goal in this homework is to develop a logistic regression\n",
        "model to predict the probability of survival to hospital discharge of these patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Up1wd2v7DhYj",
      "metadata": {
        "id": "Up1wd2v7DhYj"
      },
      "outputs": [],
      "source": [
        "# Suggested packages, you can add more if you think they are necessary.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X3iFlWRnDoDG",
      "metadata": {
        "id": "X3iFlWRnDoDG"
      },
      "outputs": [],
      "source": [
        "# Download the data. Uncomment the line below if you are using Google colab.\n",
        "# !gdown https://drive.google.com/uc?id=1dGlTA6GsxDwoRjLc9lXfB368U3QMprwL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yafwp1DBC1gg",
      "metadata": {
        "id": "yafwp1DBC1gg"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "1. Load the dataset `burn.csv` and display the first 5 rows.\n",
        "2. Print out all columns in the dataset and identify any missing values.\n",
        "3. Create a crosstab between the `tbsa` and `death` columns. Combine the least frequent category into an \"other\" category or with a category of similar meaning, and justify your decision. The new `tbsa` variable should contain three categories.\n",
        "4. Create a crosstab between the new `tbsa` and `death` columns.\n",
        "5. Create dummy variables for the new `tbsa` and display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "byg7RoRhC1gh",
      "metadata": {
        "id": "byg7RoRhC1gh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('burn.csv')\n",
        "print(\"first 5 rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\nshape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OUCLqNW9C1gh",
      "metadata": {
        "id": "OUCLqNW9C1gh"
      },
      "outputs": [],
      "source": [
        "print(\"all columns:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nnull values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\ntotal null values: {df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MQT0zcRyauEC",
      "metadata": {
        "id": "MQT0zcRyauEC"
      },
      "outputs": [],
      "source": [
        "crosstab_original = pd.crosstab(df['tbsa'], df['death'], margins=True)\n",
        "print(\"crosstab:\")\n",
        "print(crosstab_original)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DP7VDKi9cu_E",
      "metadata": {
        "id": "DP7VDKi9cu_E"
      },
      "outputs": [],
      "source": [
        "print(\"original tbsa value counts:\")\n",
        "print(df['tbsa'].value_counts())\n",
        "\n",
        "df['tbsa_new'] = df['tbsa'].replace({'critical': 'severe'})\n",
        "\n",
        "print(\"\\nnew tbsa value counts:\")\n",
        "print(df['tbsa_new'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7xJIBGN1gL5-",
      "metadata": {
        "id": "7xJIBGN1gL5-"
      },
      "outputs": [],
      "source": [
        "crosstab_new = pd.crosstab(df['tbsa_new'], df['death'], margins=True)\n",
        "print(\"crosstab:\")\n",
        "print(crosstab_new)\n",
        "\n",
        "print(\"\\njustification:\")\n",
        "print(\"combined critical with severe b/c critical has the least observations and both represent most serious burn cases with high mortality\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50MhjOEigf9e",
      "metadata": {
        "id": "50MhjOEigf9e"
      },
      "outputs": [],
      "source": [
        "tbsa_dummies = pd.get_dummies(df['tbsa_new'], prefix='tbsa', drop_first=True)\n",
        "print(\"dummy variables for tbsa_new:\")\n",
        "print(tbsa_dummies.head(10))\n",
        "print(f\"\\nshape: {tbsa_dummies.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z4fo9KJYRp5E",
      "metadata": {
        "id": "z4fo9KJYRp5E"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "1.   Check the class distribution of the `death` variable. How many positive class?\n",
        "2.   What is the baseline accuracy for this classification problem?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zedSe2liT2F1",
      "metadata": {
        "id": "zedSe2liT2F1"
      },
      "outputs": [],
      "source": [
        "print(\"class distribution of death:\")\n",
        "print(df['death'].value_counts())\n",
        "print(\"\\nclass distribution (percentages):\")\n",
        "print(df['death'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sW2gnHN6T6G2",
      "metadata": {
        "id": "sW2gnHN6T6G2"
      },
      "outputs": [],
      "source": [
        "positive_class_count = df['death'].sum()\n",
        "print(f\"number of positive class (death=1): {positive_class_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wFnmNG3KUYd0",
      "metadata": {
        "id": "wFnmNG3KUYd0"
      },
      "outputs": [],
      "source": [
        "baseline_accuracy = df['death'].value_counts(normalize=True).max()\n",
        "print(f\"baseline accuracy: {baseline_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TY8uSjkkC1gk",
      "metadata": {
        "id": "TY8uSjkkC1gk"
      },
      "source": [
        "## Question 3:\n",
        "\n",
        "Split the data into training (70%) and testing (30%) sets. **Use your student ID** as the `random_state`. Set `stratify` to the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bOQXo-DRC1gk",
      "metadata": {
        "id": "bOQXo-DRC1gk"
      },
      "outputs": [],
      "source": [
        "df_final = pd.concat([df.drop(['tbsa', 'tbsa_new'], axis=1), tbsa_dummies], axis=1)\n",
        "\n",
        "X = df_final.drop('death', axis=1)\n",
        "y = df_final['death']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=251280006, stratify=y)\n",
        "\n",
        "print(f\"training: {X_train.shape}\")\n",
        "print(f\"testing: {X_test.shape}\")\n",
        "print(f\"\\ndistribution:\")\n",
        "print(y_train.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MaiclNWgC1gl",
      "metadata": {
        "id": "MaiclNWgC1gl"
      },
      "source": [
        "## Question 4:\n",
        "\n",
        "1. Create a pipeline that first standardizes the non-binary variables using a z-scale transform, and then trains an instance of `LogisticRegression` with `penalty = None` and `max_iter = 10000`. **Use the same random seed you used before.**\n",
        "2. Train the pipeline using the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zX-AUhrpC1gl",
      "metadata": {
        "id": "zX-AUhrpC1gl"
      },
      "outputs": [],
      "source": [
        "non_binary_features = ['age']\n",
        "binary_features = ['gender', 'race', 'inh_inj', 'flame', 'tbsa_moderate', 'tbsa_severe']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaler', StandardScaler(), non_binary_features),\n",
        "        ('passthrough', 'passthrough', binary_features)\n",
        "    ])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(penalty=None, max_iter=10000, random_state=251280006))\n",
        "])\n",
        "\n",
        "print(\"pipeline created\")\n",
        "print(pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wwd_EU9Ukq9V",
      "metadata": {
        "id": "wwd_EU9Ukq9V"
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "print(\"pipeline trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Ch70_n3h2_W",
      "metadata": {
        "id": "1Ch70_n3h2_W"
      },
      "source": [
        "## Question 5:\n",
        "\n",
        "1. Display the training parameters and intercept of the logistic regression model.\n",
        "2. Compute odds ratios for all variables.\n",
        "3. Interpret the odds ratios for `age` and `inh_inj`.\n",
        "4. Predict over the test set and compute the modelâ€™s accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W-rScCkL4gRV",
      "metadata": {
        "id": "W-rScCkL4gRV"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
        "intercept = pipeline.named_steps['classifier'].intercept_[0]\n",
        "\n",
        "feature_names = non_binary_features + binary_features\n",
        "\n",
        "print(\"logistic regression coefficients:\")\n",
        "for name, coef in zip[tuple[str, Any]](feature_names, coefficients):\n",
        "    print(f\"  {name}: {coef:.4f}\")\n",
        "    \n",
        "print(f\"\\nintercept: {intercept:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Ey2JVxA1Rr4",
      "metadata": {
        "id": "6Ey2JVxA1Rr4"
      },
      "outputs": [],
      "source": [
        "odds_ratios = np.exp(coefficients)\n",
        "\n",
        "print(\"odds ratios:\")\n",
        "for name, or_val in zip(feature_names, odds_ratios):\n",
        "    print(f\"  {name}: {or_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HvVly4CN8rf7",
      "metadata": {
        "id": "HvVly4CN8rf7"
      },
      "source": [
        "Interpret the odds ratio for `age`.\n",
        "\n",
        "for every one unit increase in age (after standardization), the odds of death increase by a factor of the odds ratio. since age is standardized, this represents the effect of a one standard deviation increase in age. older patients have higher odds of death.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EIQEV-KFCpUI",
      "metadata": {
        "id": "EIQEV-KFCpUI"
      },
      "source": [
        "Interpret the odds ratio `inh_inj`.\n",
        "\n",
        "patients with inhalation injury have odds of death that are the odds ratio times higher than patients without inhalation injury, holding all other variables constant. inhalation injury significantly increases the risk of death."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eyXmnFjRL7D4",
      "metadata": {
        "id": "eyXmnFjRL7D4"
      },
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"model accuracy on test: {accuracy:.4f}\")\n",
        "print(\"\\nconfusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nclassification report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lYiUpH_YC1gm",
      "metadata": {
        "id": "lYiUpH_YC1gm"
      },
      "source": [
        "## Question 6:\n",
        "\n",
        "1. Plot the ROC curve to see the performance over all cutoffs.\n",
        "2. Compute the area under the curve (AUC). Is the AUC acceptable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9KVu82JLC1gn",
      "metadata": {
        "id": "9KVu82JLC1gn"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, 'b-', linewidth=2, label='roc curve')\n",
        "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='random classifier')\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.title('roc curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lzffPyNec28E",
      "metadata": {
        "id": "lzffPyNec28E"
      },
      "outputs": [],
      "source": [
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"auc: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pkECJO3JeBk2",
      "metadata": {
        "id": "pkECJO3JeBk2"
      },
      "source": [
        "Is the AUC acceptable?\n",
        "\n",
        "yes, the auc is acceptable. an auc above 0.8 is generally considered good, and above 0.9 is excellent. this model shows strong discriminative ability in predicting patient survival."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
